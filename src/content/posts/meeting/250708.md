---
title: meeting-250708
published: 2025-07-08
tags: []
category: Meeting
draft: false
---

# Word2Vec

[발표자료](https://www.notion.so/Efficient-Estimation-of-Word-Representations-in-Vector-Space-22a5b55ab86d8080bef8e0bf23e5bef3)

Word to Vector

왜 씀? 단어들 사이의 관계가 벡터의 덧셈과 뺄셈으로 표현됨

## 방법

- CBOW (Continuous Bag of Word)
- Skip-gram

## 최적화

NNLM, RNN 등보다 훨씬 빠름 (은닉층이 없음)

계층적 softmax -> Huffman tree 사용해서 log V번 쿼리&공간으로 단어 찾기 + 자주 사용되는 단어는 루트 가깝게

# Transformer

[발표자료](https://docs.google.com/presentation/d/1dYFySRGyqI1808pEKTnTTnDCN_RgK7ZwtGUeHQwwz9o/edit?slide=id.p#slide=id.p)

## Motive

RNN + Attention, CNN + Attention 있었음  
But RNN, CNN 구림 -> Attention만 써보죠?

## Structure

Encoder, decoder가 있음

## Attention

- Encoder attention
- Decoder attention
  - Mask로 이전 토큰들의 attention은 무시함
- Encoder-Decoder attention

## Embedding

Positional embedding - sinusoid 사용

$PE_{pos+k}$가 $PE_{pos}$의 linear function이 되기 때문에 relative position에 대한 attention이 잘 계산될 거라는 기대가 있음

## Result

빠르다! 좋다!

실제로 multi-head attention이 문장의 서로 다른 문맥적 구조를 파악한다.

# BERT

[발표자료1](https://docs.google.com/presentation/d/1dYFySRGyqI1808pEKTnTTnDCN_RgK7ZwtGUeHQwwz9o/edit?slide=id.p#slide=id.p)  
[발표자료2](https://www.notion.so/BERT-225b9cc1472d80ab82f0f38b91d97087)

- 각 토큰이 양방향 참조를 할 수 있음! (이전까진 LTR RTL처럼 방향을 나눴음)
- 각 토큰이 전체 시퀀스에서 주어진 양방향 문맥을 학습할 수 있음

Attention masking으로는 해결이 안 됨 -> 마스크 토큰을 따로 만들어서 문장으로 집어넣음

## 전이학습 (Transition learning)

Layer (Attention 부분), Head (Attention 통과된걸 output으로 바꿔줌)

- Feature-based approach
  - Layer 놔두고 head만 바꿈
- Fine-tuning approach
  - Layer와 head를 둘 다 바꿈

보통 Fine-tuning을 씀  
왜냐 Feature-based는 head가 학습되려면 layer만큼 깊이가 있어야 하기 때문에 Fine-tuning이 더 빠르고 성능도 잘 나옴

## Embedding

Bert는 subword로 tokenize함 (e.g. singing -> sing + ##ing)

여러 special token도 존재함

- `[CLS]`: 시작할 때 반드시 넣음 (Last hidden states에서 문장 전체를 나타내는 hidden state가 됨)
- `[SEP]`: 문장 사이에 넣음
- `[MASK]`: 마스크용

## Training

- MLM: 일정 확률로 masking, 무작위 token으로 변경 or 그대로 유지
- NSP: 두 문장을 일정 확률로 연관 있는 다음 문장 or 연관 없는 아무 문장

# 다음 주에 하기

- r-vector-visualization 왜 안 끝나냐고
- [논문 읽기](https://docs.google.com/spreadsheets/d/1LD055uiXTapTUYuWxKmzTIAJwTc_4foeiYZEAKCb6ME/edit?usp=sharing) 스프레드시트 가져가서 각자 논문 20개씩 읽고 정리하기?  
  목표: 우리 주제랑 관련 있는거
